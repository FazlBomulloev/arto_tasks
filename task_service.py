import asyncio
import logging
import time
import random
import math
import json
from typing import List, Dict, Optional
from dataclasses import dataclass
from enum import Enum

from config import (
    BATCH_SIZE, read_setting, 
    find_english_word, find_lang_code
)
from database import get_accounts_by_lang, get_channels_by_lang
from session_manager import global_session_manager
from exceptions import TaskProcessingError

logger = logging.getLogger(__name__)

class TaskType(Enum):
    VIEW = "view"
    SUBSCRIBE = "subscribe"

@dataclass
class TaskItem:
    account_session: str
    phone: str
    channel: str  
    lang: str
    task_type: TaskType
    post_id: Optional[int] = None
    execute_at: Optional[float] = None
    retry_count: int = 0

class ShardedTaskService:

    def __init__(self):
        self.batch_size = BATCH_SIZE
        self.view_shard_interval = 15 * 60  
        self.subscription_shard_interval = 60 * 60 
        
    def get_view_duration(self) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        hours = read_setting('followPeriod.txt', 10.0)  # –î–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º 10 —á–∞—Å–æ–≤
        return int(hours * 3600)
        
    async def create_view_tasks_for_post(self, channel_username: str, post_id: int) -> Dict[str, int]:
        """
        –°–æ–∑–¥–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å –®–ê–†–î–ò–ù–ì–û–ú –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á
        """
        results = {
            'total_tasks': 0,
            'batches_created': 0,
            'shards_created': 0,
            'languages': 0
        }
        
        try:
            # ‚úÖ –ß–ò–¢–ê–ï–ú –ê–ö–¢–£–ê–õ–¨–ù–£–Æ –î–õ–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ò–ó –ù–ê–°–¢–†–û–ï–ö
            view_duration = self.get_view_duration()
            view_hours = view_duration / 3600
            
            logger.info(f"üìä –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {view_hours} —á–∞—Å–æ–≤ (–∏–∑ followPeriod.txt)")
            
            # 1. –ü–æ–ª—É—á–∞–µ–º —è–∑—ã–∫–∏ –∫–∞–Ω–∞–ª–∞ –∏–∑ –ë–î
            languages = await self._get_channel_languages(channel_username)
            if not languages:
                logger.error(f"‚õî –ö–∞–Ω–∞–ª @{channel_username} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î")
                return results
            
            results['languages'] = len(languages)
            all_tasks = []
            
            # 2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ–ª—É—á–∞–µ–º –∞–∫–∫–∞—É–Ω—Ç—ã
            for lang in languages:
                english_lang = find_english_word(lang)
                accounts = await get_accounts_by_lang(english_lang, 'active')
                
                if not accounts:
                    logger.warning(f"‚ö† –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–∞ {lang}")
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–æ–≤ —ç—Ç–æ–≥–æ —è–∑—ã–∫–∞
                for account in accounts:
                    task = TaskItem(
                        account_session=account['session_data'],
                        phone=account['phone_number'],
                        channel=channel_username,
                        lang=english_lang,
                        task_type=TaskType.VIEW,
                        post_id=post_id
                    )
                    all_tasks.append(task)
            
            results['total_tasks'] = len(all_tasks)
            
            if not all_tasks:
                logger.warning("‚ö† –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è")
                return results
            
            # 3. –°–æ–∑–¥–∞–µ–º –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–ï –±–∞—Ç—á–∏
            batches, shards_count = await self._create_sharded_view_batches(all_tasks, post_id, view_duration)
            results['batches_created'] = len(batches)
            results['shards_created'] = shards_count
            
            logger.info(f"""
üìä –°–æ–∑–¥–∞–Ω–æ –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–• –∑–∞–¥–∞—á –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–ª—è –ø–æ—Å—Ç–∞ {post_id}:
   üì± –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {results['total_tasks']}
   üåê –Ø–∑—ã–∫–æ–≤: {results['languages']}  
   üì¶ –ë–∞—Ç—á–µ–π: {results['batches_created']}
   üóÇÔ∏è –®–∞—Ä–¥–æ–≤: {results['shards_created']}
   ‚è∞ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {view_hours} —á–∞—Å–æ–≤
            """)
            
            return results
            
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–æ—Å–º–æ—Ç—Ä–∞: {e}")
            raise TaskProcessingError(f"Failed to create sharded view tasks: {e}")
    
    async def _get_channel_languages(self, channel_username: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —è–∑—ã–∫–∏ –∫–∞–Ω–∞–ª–∞ –∏–∑ –ë–î"""
        try:
            from database import db_session
            async with db_session() as conn:
                results = await conn.fetch(
                    'SELECT DISTINCT lang FROM channels WHERE name = $1',
                    channel_username
                )
                return [result['lang'] for result in results]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤ –∫–∞–Ω–∞–ª–∞: {e}")
            return []
    
    async def _create_sharded_view_batches(self, all_tasks: List[TaskItem], post_id: int, duration_seconds: int) -> tuple[List[Dict], int]:
        if not all_tasks:
            return [], 0
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏
        random.shuffle(all_tasks)
        
        total_accounts = len(all_tasks)
        
        logger.info(f"üìä –ü–æ—Å—Ç {post_id}: –¢–û–ß–ù–û–ï —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_accounts} –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –Ω–∞ {duration_seconds/3600:.1f} —á–∞—Å–æ–≤")
        
        # ‚úÖ –¢–û–ß–ù–´–ô —Ä–∞—Å—á–µ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        if total_accounts > 1:
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ –≤—Å–µ–º—É –ø–µ—Ä–∏–æ–¥—É
            interval_per_account = duration_seconds / total_accounts
        else:
            interval_per_account = 0
        
        interval_per_account = max(interval_per_account, 30)
        
        if interval_per_account > 300:  # –ë–æ–ª—å—à–µ 5 –º–∏–Ω—É—Ç
            interval_per_account = 300
            logger.info(f"‚ö° –£—Å–∫–æ—Ä—è—é –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–æ 5 –º–∏–Ω—É—Ç –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏")
        
        logger.info(f"‚è±Ô∏è –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∞–∫–∫–∞—É–Ω—Ç–∞–º–∏: {interval_per_account:.1f} —Å–µ–∫—É–Ω–¥ ({interval_per_account/60:.1f} –º–∏–Ω)")
        
        # ‚úÖ –°–û–ó–î–ê–ï–ú –ó–ê–î–ê–ß–ò –° –¢–û–ß–ù–´–ú –í–†–ï–ú–ï–ù–ï–ú
        current_time = time.time()
        batches = []
        shards_map = {}
        
        for idx, task in enumerate(all_tasks):
          
            execute_at = current_time + (idx * interval_per_account)
            
            max_time = current_time + duration_seconds - 60  
            if execute_at > max_time:
                execute_at = max_time - (total_accounts - idx - 1) * 30  
            
            randomization_range = min(interval_per_account * 0.1, 30) 
            randomization = random.uniform(-randomization_range, randomization_range)
            execute_at += randomization
            
            execute_at = max(execute_at, current_time + 30) 
            execute_at = min(execute_at, current_time + duration_seconds - 30)  
            shard_interval = 900 
            shard_id = int((execute_at - current_time) // shard_interval)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            task.execute_at = execute_at
            
            batch_info = {
                'batch_number': idx + 1,
                'tasks': [task],
                'execute_at': execute_at,
                'delay_minutes': (execute_at - current_time) / 60,
                'account_phone': task.phone,
                'post_id': post_id,
                'shard_id': shard_id
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —à–∞—Ä–¥
            if shard_id not in shards_map:
                shards_map[shard_id] = []
            shards_map[shard_id].append(batch_info)
            
            batches.append(batch_info)
        
        # ‚úÖ –ü–†–û–í–ï–†–ö–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø
        first_time = min(batch['execute_at'] for batch in batches)
        last_time = max(batch['execute_at'] for batch in batches)
        actual_duration = (last_time - first_time) / 3600
        
        logger.info(f"""
    ‚úÖ –ü–æ—Å—Ç {post_id}: —Å–æ–∑–¥–∞–Ω–æ {len(batches)} –∑–∞–¥–∞—á –≤ {len(shards_map)} —à–∞—Ä–¥–∞—Ö
    ‚è∞ –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä: —á–µ—Ä–µ–∑ {(first_time - current_time)/60:.1f} –º–∏–Ω
    ‚è∞ –ü–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ—Å–º–æ—Ç—Ä: —á–µ—Ä–µ–∑ {(last_time - current_time)/60:.1f} –º–∏–Ω  
    üìä –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–∏–æ–¥: {actual_duration:.2f} —á–∞—Å–æ–≤
    üéØ –¶–µ–ª–µ–≤–æ–π –ø–µ—Ä–∏–æ–¥: {duration_seconds/3600:.1f} —á–∞—Å–æ–≤
    ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {abs(actual_duration - duration_seconds/3600) < 0.1}
        """)
        

        await self._schedule_sharded_batches(shards_map, post_id, current_time, shard_interval)
        
        return batches, len(shards_map)
    
    async def _schedule_sharded_batches(self, shards_map: Dict[int, List[Dict]], post_id: int, 
                                      base_time: float, shard_interval: int):
        """–ü–ª–∞–Ω–∏—Ä—É–µ—Ç –±–∞—Ç—á–∏ –≤ –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–ï –æ—á–µ—Ä–µ–¥–∏"""
        try:
            from redis import Redis
            from config import REDIS_HOST, REDIS_PORT, REDIS_PASSWORD
            
            redis_client = Redis(
                host=REDIS_HOST,
                port=REDIS_PORT, 
                password=REDIS_PASSWORD,
                decode_responses=True
            )
            
            logger.info(f"üìã –ü–ª–∞–Ω–∏—Ä—É—é {len(shards_map)} —à–∞—Ä–¥–æ–≤ –¥–ª—è –ø–æ—Å—Ç–∞ {post_id}")
            
            total_scheduled = 0
            
            for shard_id, shard_batches in shards_map.items():
                # ‚úÖ –ö–õ–Æ–ß –®–ê–†–î–ê –≤–∫–ª—é—á–∞–µ—Ç –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                shard_start_time = base_time + (shard_id * shard_interval)
                shard_key = f"view_shard_{int(shard_start_time)}_{shard_id}"
                
                # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —à–∞—Ä–¥–∞
                shard_data = {}
                for batch_info in shard_batches:
                    batch_data = {
                        'batch_number': batch_info['batch_number'],
                        'execute_at': batch_info['execute_at'],
                        'post_id': post_id,
                        'task_type': 'view',
                        'shard_id': shard_id,
                        'tasks': [
                            {
                                'account_session': task.account_session,
                                'phone': task.phone,
                                'channel': task.channel,
                                'lang': task.lang,
                                'post_id': task.post_id,
                                'execute_at': task.execute_at
                            }
                            for task in batch_info['tasks']
                        ]
                    }
                    
                    shard_data[json.dumps(batch_data)] = batch_info['execute_at']
                
                # ‚úÖ –ó–ê–ü–ò–°–´–í–ê–ï–ú –®–ê–†–î –≤ Redis
                if shard_data:
                    redis_client.zadd(shard_key, shard_data)
                    
                    # TTL –Ω–∞ 24 —á–∞—Å–∞
                    redis_client.expire(shard_key, 24 * 3600)
                    
                    total_scheduled += len(shard_batches)
                    
                    logger.info(f"  üì¶ –®–∞—Ä–¥ {shard_id}: {len(shard_batches)} –±–∞—Ç—á–µ–π ‚Üí {shard_key}")
            
            # ‚úÖ –†–ï–ì–ò–°–¢–†–ò–†–£–ï–ú –ê–ö–¢–ò–í–ù–´–ï –®–ê–†–î–´ –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞
            await self._register_active_shards(redis_client, shards_map.keys(), base_time, shard_interval, post_id)
            
            logger.info(f"‚úÖ –ü–æ—Å—Ç {post_id}: {total_scheduled} –±–∞—Ç—á–µ–π –≤ {len(shards_map)} —à–∞—Ä–¥–∞—Ö")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π: {e}")
            raise TaskProcessingError(f"Failed to schedule sharded batches: {e}")
    
    async def _register_active_shards(self, redis_client, shard_ids: List[int], 
                                    base_time: float, shard_interval: int, post_id: int):
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ —à–∞—Ä–¥—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–æ—Ä–∫–µ—Ä–æ–º"""
        try:
            active_shards_key = "active_view_shards"
            
            for shard_id in shard_ids:
                shard_start_time = base_time + (shard_id * shard_interval)
                shard_key = f"view_shard_{int(shard_start_time)}_{shard_id}"
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —à–∞—Ä–¥–∞ –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞
                shard_meta = {
                    'shard_key': shard_key,
                    'shard_id': shard_id,
                    'start_time': shard_start_time,
                    'end_time': shard_start_time + shard_interval,
                    'post_id': post_id,
                    'type': 'view'
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–µ—Å—Ç—Ä –∞–∫—Ç–∏–≤–Ω—ã—Ö —à–∞—Ä–¥–æ–≤ (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞)
                redis_client.zadd(active_shards_key, {json.dumps(shard_meta): shard_start_time})
            
            # TTL –Ω–∞ 48 —á–∞—Å–æ–≤
            redis_client.expire(active_shards_key, 48 * 3600)
            
            logger.info(f"üìã –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(shard_ids)} –∞–∫—Ç–∏–≤–Ω—ã—Ö —à–∞—Ä–¥–æ–≤")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö —à–∞—Ä–¥–æ–≤: {e}")
    
    async def create_subscription_tasks(self, channel_name: str, target_lang: str) -> Dict[str, int]:
        """
        –°–æ–∑–¥–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ–¥–ø–∏—Å–∫–∏ —Å –®–ê–†–î–ò–ù–ì–û–ú –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á
        """
        results = {
            'total_tasks': 0,
            'accounts_processed': 0,
            'shards_created': 0
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫–∫–∞—É–Ω—Ç—ã –¥–ª—è –ø–æ–¥–ø–∏—Å–∫–∏
            english_lang = find_english_word(target_lang)
            accounts = await get_accounts_by_lang(english_lang, 'active')
            
            if not accounts:
                logger.warning(f"‚ö† –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–∞ {target_lang}")
                return results
            
            results['accounts_processed'] = len(accounts)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–µ—Ä–∂–µ–∫ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            params = await self._get_subscription_delays()
            
            logger.info(f"""
üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–• –ø–æ–¥–ø–∏—Å–æ–∫ –¥–ª—è –∫–∞–Ω–∞–ª–∞ @{channel_name}:
   üì± –ê–∫–∫–∞—É–Ω—Ç–æ–≤: {len(accounts)}
   ‚è∞ –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {params['base_delay']/60:.1f} –º–∏–Ω
   üé≤ –†–∞–∑–±—Ä–æ—Å: ¬±{params['range_val']/60:.1f} –º–∏–Ω
   üë• –ú–µ–∂–¥—É –∞–∫–∫–∞—É–Ω—Ç–∞–º–∏: {params['accounts_delay']/60:.1f} –º–∏–Ω
   üî¢ –ü–æ–¥–ø–∏—Å–æ–∫ –¥–æ –ø–∞—É–∑—ã: {params['timeout_count']}
   ‚è∏Ô∏è –ü–∞—É–∑–∞: {params['timeout_duration']/60:.1f} –º–∏–Ω
   üóÇÔ∏è –®–∞—Ä–¥–∏–Ω–≥: {self.subscription_shard_interval//60} –º–∏–Ω –Ω–∞ —à–∞—Ä–¥
            """)
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∞–∫–∫–∞—É–Ω—Ç—ã –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏
            random.shuffle(accounts)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ —Å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
            subscription_tasks = []
            current_time = time.time()
            
            for account_idx, account in enumerate(accounts):
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¢–û–ß–ù–£–Æ –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —ç—Ç–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞
                delay_seconds = await self._calculate_precise_subscription_delay(
                    account_idx, params
                )
                
                execute_at = current_time + delay_seconds
                
                task = TaskItem(
                    account_session=account['session_data'],
                    phone=account['phone_number'],
                    channel=channel_name,
                    lang=english_lang,
                    task_type=TaskType.SUBSCRIBE,
                    execute_at=execute_at
                )
                
                subscription_tasks.append(task)
            
            results['total_tasks'] = len(subscription_tasks)
            
            # ‚úÖ –ü–õ–ê–ù–ò–†–£–ï–ú –í –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–• –û–ß–ï–†–ï–î–Ø–•
            shards_count = await self._schedule_sharded_subscription_tasks(subscription_tasks, channel_name)
            results['shards_created'] = shards_count
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            total_duration_hours = max(task.execute_at - current_time for task in subscription_tasks) / 3600
            
            logger.info(f"""
‚úÖ –°–æ–∑–¥–∞–Ω–æ {results['total_tasks']} –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–• –∑–∞–¥–∞—á –ø–æ–¥–ø–∏—Å–∫–∏:
   üìÖ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {total_duration_hours:.1f} —á–∞—Å–æ–≤
   üóÇÔ∏è –®–∞—Ä–¥–æ–≤: {results['shards_created']}
   ‚ö° –ü–µ—Ä–≤–∞—è –ø–æ–¥–ø–∏—Å–∫–∞: —á–µ—Ä–µ–∑ {min(task.execute_at - current_time for task in subscription_tasks)/60:.1f} –º–∏–Ω
   üèÅ –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–¥–ø–∏—Å–∫–∞: —á–µ—Ä–µ–∑ {total_duration_hours:.1f} —á–∞—Å–æ–≤
            """)
            
            return results
            
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–¥–ø–∏—Å–∫–∏: {e}")
            raise TaskProcessingError(f"Failed to create sharded subscription tasks: {e}")
    
    async def _get_subscription_delays(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ê–ö–¢–£–ê–õ–¨–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–µ—Ä–∂–µ–∫ –ø–æ–¥–ø–∏—Å–æ–∫ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        params = {
            'base_delay': read_setting('lag.txt', 30.0) * 60,      
            'range_val': read_setting('range.txt', 5.0) * 60,      
            'accounts_delay': read_setting('accounts_delay.txt', 10.0) * 60,
            'timeout_count': int(read_setting('timeout_count.txt', 4.0)),
            'timeout_duration': read_setting('timeout_duration.txt', 20.0) * 60
        }
        
        logger.info(f"""
üìã –ü—Ä–æ—á–∏—Ç–∞–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–ø–∏—Å–æ–∫ –∏–∑ vars/:
   üìÖ –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ (lag.txt): {params['base_delay']/60:.1f} –º–∏–Ω
   üé≤ –†–∞–∑–±—Ä–æ—Å (range.txt): {params['range_val']/60:.1f} –º–∏–Ω
   üë• –ú–µ–∂–¥—É –∞–∫–∫–∞—É–Ω—Ç–∞–º–∏ (accounts_delay.txt): {params['accounts_delay']/60:.1f} –º–∏–Ω
   üî¢ –ü–æ–¥–ø–∏—Å–æ–∫ –¥–æ –ø–∞—É–∑—ã (timeout_count.txt): {params['timeout_count']}
   ‚è∏Ô∏è –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—É–∑—ã (timeout_duration.txt): {params['timeout_duration']/60:.1f} –º–∏–Ω
        """)
        
        return params
    
    async def _calculate_precise_subscription_delay(self, account_index: int, params: Dict[str, float]) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –¢–û–ß–ù–£–Æ –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –ø–æ–¥–ø–∏—Å–∫–∏ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò –ø–∞—É–∑–∞–º–∏
        """
        base_delay = params['base_delay']           # –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ (—Å–µ–∫—É–Ω–¥—ã)
        range_val = params['range_val']             # –†–∞–∑–±—Ä–æ—Å (—Å–µ–∫—É–Ω–¥—ã)
        accounts_delay = params['accounts_delay']   # –ú–µ–∂–¥—É –∞–∫–∫–∞—É–Ω—Ç–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        timeout_count = params['timeout_count']     # –ü–æ–¥–ø–∏—Å–æ–∫ –¥–æ –ø–∞—É–∑—ã
        timeout_duration = params['timeout_duration'] # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—É–∑—ã (—Å–µ–∫—É–Ω–¥—ã)
        
        # 1. –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∞–∫–∫–∞—É–Ω—Ç–∞–º–∏
        account_delay = account_index * accounts_delay
        
        # 2. –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ä–∞–∑–±—Ä–æ—Å –∫ –∫–∞–∂–¥–æ–º—É –∞–∫–∫–∞—É–Ω—Ç—É
        random_variation = random.uniform(-range_val, range_val)
        
        # 3. ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô —Ä–∞—Å—á–µ—Ç –ø–∞—É–∑
        # –ö–∞–∂–¥—ã–µ timeout_count –ø–æ–¥–ø–∏—Å–æ–∫ –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—É–∑—É timeout_duration
        timeout_cycles = account_index // timeout_count
        timeout_delay = timeout_cycles * timeout_duration
        
        # 4. –ò—Ç–æ–≥–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        total_delay = account_delay + random_variation + timeout_delay
        
        # 5. –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ - –Ω–µ –º–µ–Ω–µ–µ –±–∞–∑–æ–≤–æ–π
        total_delay = max(total_delay, base_delay)
        
        return total_delay
    
    async def _schedule_sharded_subscription_tasks(self, tasks: List[TaskItem], channel_name: str) -> int:
        """–ü–ª–∞–Ω–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ–¥–ø–∏—Å–∫–∏ –≤ –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–• –æ—á–µ—Ä–µ–¥—è—Ö"""
        try:
            from redis import Redis
            from config import REDIS_HOST, REDIS_PORT, REDIS_PASSWORD
            
            redis_client = Redis(
                host=REDIS_HOST,
                port=REDIS_PORT,
                password=REDIS_PASSWORD, 
                decode_responses=True
            )
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            tasks.sort(key=lambda x: x.execute_at)
            
            logger.info(f"üìã –ü–ª–∞–Ω–∏—Ä—É—é {len(tasks)} –∑–∞–¥–∞—á –ø–æ–¥–ø–∏—Å–∫–∏ –≤ –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–• –æ—á–µ—Ä–µ–¥—è—Ö:")
            
            # ‚úÖ –ì–†–£–ü–ü–ò–†–£–ï–ú –ó–ê–î–ê–ß–ò –ü–û –®–ê–†–î–ê–ú
            current_time = time.time()
            shards_map = {}
            
            for task in tasks:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∞—Ä–¥ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                time_offset = task.execute_at - current_time
                shard_id = int(time_offset // self.subscription_shard_interval)
                
                if shard_id not in shards_map:
                    shards_map[shard_id] = []
                shards_map[shard_id].append(task)
            
            # ‚úÖ –°–û–ó–î–ê–ï–ú –®–ê–†–î–´ –ò –ó–ê–ü–ò–°–´–í–ê–ï–ú –í REDIS
            scheduled_count = 0
            
            for shard_id, shard_tasks in shards_map.items():
                shard_start_time = current_time + (shard_id * self.subscription_shard_interval)
                shard_key = f"sub_shard_{int(shard_start_time)}_{shard_id}"
                
                shard_data = {}
                
                for task in shard_tasks:
                    task_data = {
                        'account_session': task.account_session,
                        'phone': task.phone,
                        'channel': task.channel,
                        'lang': task.lang,
                        'task_type': 'subscription',
                        'execute_at': task.execute_at,
                        'retry_count': task.retry_count,
                        'created_at': time.time(),
                        'shard_id': shard_id
                    }
                    
                    shard_data[json.dumps(task_data)] = task.execute_at
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —à–∞—Ä–¥ –≤ Redis
                if shard_data:
                    redis_client.zadd(shard_key, shard_data)
                    redis_client.expire(shard_key, 48 * 3600)  # TTL 48 —á–∞—Å–æ–≤
                    
                    scheduled_count += len(shard_tasks)
                    
                    avg_delay = sum(t.execute_at - current_time for t in shard_tasks) / len(shard_tasks) / 60
                    logger.info(f"  üì¶ –®–∞—Ä–¥ {shard_id}: {len(shard_tasks)} –∑–∞–¥–∞—á (—Å—Ä–µ–¥–Ω–µ–µ: {avg_delay:.1f} –º–∏–Ω)")
            
            # ‚úÖ –†–ï–ì–ò–°–¢–†–ò–†–£–ï–ú –ê–ö–¢–ò–í–ù–´–ï –®–ê–†–î–´ –ü–û–î–ü–ò–°–û–ö
            await self._register_active_subscription_shards(redis_client, shards_map.keys(), 
                                                          current_time, channel_name)
            
            logger.info(f"‚úÖ –ö–∞–Ω–∞–ª @{channel_name}: {scheduled_count} –∑–∞–¥–∞—á –≤ {len(shards_map)} —à–∞—Ä–¥–∞—Ö")
            
            return len(shards_map)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–¥–ø–∏—Å–∫–∏: {e}")
            raise TaskProcessingError(f"Failed to schedule sharded subscription tasks: {e}")
    
    async def _register_active_subscription_shards(self, redis_client, shard_ids: List[int], 
                                                 base_time: float, channel_name: str):
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ —à–∞—Ä–¥—ã –ø–æ–¥–ø–∏—Å–æ–∫"""
        try:
            active_shards_key = "active_subscription_shards"
            
            for shard_id in shard_ids:
                shard_start_time = base_time + (shard_id * self.subscription_shard_interval)
                shard_key = f"sub_shard_{int(shard_start_time)}_{shard_id}"
                
                shard_meta = {
                    'shard_key': shard_key,
                    'shard_id': shard_id,
                    'start_time': shard_start_time,
                    'end_time': shard_start_time + self.subscription_shard_interval,
                    'channel': channel_name,
                    'type': 'subscription'
                }
                
                redis_client.zadd(active_shards_key, {json.dumps(shard_meta): shard_start_time})
            
            redis_client.expire(active_shards_key, 48 * 3600)
            
            logger.info(f"üìã –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(shard_ids)} –∞–∫—Ç–∏–≤–Ω—ã—Ö —à–∞—Ä–¥–æ–≤ –ø–æ–¥–ø–∏—Å–æ–∫")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ —à–∞—Ä–¥–æ–≤ –ø–æ–¥–ø–∏—Å–æ–∫: {e}")
    
    async def get_task_stats(self) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–¥–∞—á –∏–∑ –®–ê–†–î–ò–†–û–í–ê–ù–ù–´–• –æ—á–µ—Ä–µ–¥–µ–π"""
        try:
            from redis import Redis
            from config import REDIS_HOST, REDIS_PORT, REDIS_PASSWORD
            
            redis_client = Redis(
                host=REDIS_HOST,
                port=REDIS_PORT,
                password=REDIS_PASSWORD,
                decode_responses=True
            )
            
            current_time = time.time()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —à–∞—Ä–¥–∞–º –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
            view_shards = redis_client.zcount("active_view_shards", 0, "+inf")
            view_ready_shards = redis_client.zcount("active_view_shards", 0, current_time + 1800)  # +30 –º–∏–Ω
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —à–∞—Ä–¥–∞–º –ø–æ–¥–ø–∏—Å–æ–∫
            sub_shards = redis_client.zcount("active_subscription_shards", 0, "+inf")
            sub_ready_shards = redis_client.zcount("active_subscription_shards", 0, current_time + 3600)  # +1 —á–∞—Å
            
            # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–¥–∞—á (–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞—Ä–¥–æ–≤)
            estimated_view_tasks = 0
            estimated_sub_tasks = 0
            
            # –ë–µ—Ä–µ–º –æ–±—Ä–∞–∑—Ü—ã –∞–∫—Ç–∏–≤–Ω—ã—Ö —à–∞—Ä–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            sample_view_shards = redis_client.zrange("active_view_shards", 0, 5)
            for shard_meta_json in sample_view_shards:
                try:
                    shard_meta = json.loads(shard_meta_json)
                    shard_size = redis_client.zcard(shard_meta['shard_key'])
                    estimated_view_tasks += shard_size
                except:
                    pass
            
            sample_sub_shards = redis_client.zrange("active_subscription_shards", 0, 5)
            for shard_meta_json in sample_sub_shards:
                try:
                    shard_meta = json.loads(shard_meta_json)
                    shard_size = redis_client.zcard(shard_meta['shard_key'])
                    estimated_sub_tasks += shard_size
                except:
                    pass
            
            stats = {
                'view_shards_total': view_shards,
                'view_shards_ready': view_ready_shards,
                'subscription_shards_total': sub_shards,
                'subscription_shards_ready': sub_ready_shards,
                'estimated_view_tasks': estimated_view_tasks,
                'estimated_subscription_tasks': estimated_sub_tasks,
                'retry_queue': redis_client.llen("retry_tasks"),
                'session_pool_size': len(global_session_manager.clients),
                'session_pool_loaded': global_session_manager.loading_complete
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á: {e}")
            return {}
    
    async def cleanup_expired_shards(self) -> Dict[str, int]:
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–µ–∫—à–∏–µ —à–∞—Ä–¥—ã –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        try:
            from redis import Redis
            from config import REDIS_HOST, REDIS_PORT, REDIS_PASSWORD
            
            redis_client = Redis(
                host=REDIS_HOST,
                port=REDIS_PORT,
                password=REDIS_PASSWORD,
                decode_responses=True
            )
            
            current_time = time.time()
            cleanup_stats = {
                'expired_view_shards': 0,
                'expired_subscription_shards': 0,
                'freed_memory_mb': 0
            }
            
            # –û—á–∏—â–∞–µ–º –∏—Å—Ç–µ–∫—à–∏–µ —à–∞—Ä–¥—ã –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ (—Å—Ç–∞—Ä—à–µ 12 —á–∞—Å–æ–≤)
            expired_view_threshold = current_time - (12 * 3600)
            expired_view_shards = redis_client.zrangebyscore(
                "active_view_shards", 
                0, 
                expired_view_threshold,
                withscores=False
            )
            
            for shard_meta_json in expired_view_shards:
                try:
                    shard_meta = json.loads(shard_meta_json)
                    shard_key = shard_meta['shard_key']
                    
                    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
                    shard_size = redis_client.zcard(shard_key)
                    cleanup_stats['freed_memory_mb'] += shard_size * 0.5 / 1024  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                    
                    # –£–¥–∞–ª—è–µ–º —à–∞—Ä–¥
                    redis_client.delete(shard_key)
                    redis_client.zrem("active_view_shards", shard_meta_json)
                    
                    cleanup_stats['expired_view_shards'] += 1
                    
                    logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –∏—Å—Ç–µ–∫—à–∏–π —à–∞—Ä–¥ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {shard_key} ({shard_size} –∑–∞–¥–∞—á)")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —à–∞—Ä–¥–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {e}")
            
            # –û—á–∏—â–∞–µ–º –∏—Å—Ç–µ–∫—à–∏–µ —à–∞—Ä–¥—ã –ø–æ–¥–ø–∏—Å–æ–∫ (—Å—Ç–∞—Ä—à–µ 48 —á–∞—Å–æ–≤)
            expired_sub_threshold = current_time - (48 * 3600)
            expired_sub_shards = redis_client.zrangebyscore(
                "active_subscription_shards", 
                0, 
                expired_sub_threshold,
                withscores=False
            )
            
            for shard_meta_json in expired_sub_shards:
                try:
                    shard_meta = json.loads(shard_meta_json)
                    shard_key = shard_meta['shard_key']
                    
                    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
                    shard_size = redis_client.zcard(shard_key)
                    cleanup_stats['freed_memory_mb'] += shard_size * 0.5 / 1024
                    
                    # –£–¥–∞–ª—è–µ–º —à–∞—Ä–¥
                    redis_client.delete(shard_key)
                    redis_client.zrem("active_subscription_shards", shard_meta_json)
                    
                    cleanup_stats['expired_subscription_shards'] += 1
                    
                    logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –∏—Å—Ç–µ–∫—à–∏–π —à–∞—Ä–¥ –ø–æ–¥–ø–∏—Å–æ–∫: {shard_key} ({shard_size} –∑–∞–¥–∞—á)")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —à–∞—Ä–¥–∞ –ø–æ–¥–ø–∏—Å–æ–∫: {e}")
            
            if cleanup_stats['expired_view_shards'] > 0 or cleanup_stats['expired_subscription_shards'] > 0:
                logger.info(f"""
üßπ –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–µ–∫—à–∏—Ö —à–∞—Ä–¥–æ–≤:
   üëÄ –®–∞—Ä–¥–æ–≤ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {cleanup_stats['expired_view_shards']}
   üì∫ –®–∞—Ä–¥–æ–≤ –ø–æ–¥–ø–∏—Å–æ–∫: {cleanup_stats['expired_subscription_shards']}
   üíæ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ –ø–∞–º—è—Ç–∏: {cleanup_stats['freed_memory_mb']:.1f} –ú–ë
                """)
            
            return cleanup_stats
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–µ–∫—à–∏—Ö —à–∞—Ä–¥–æ–≤: {e}")
            return {'expired_view_shards': 0, 'expired_subscription_shards': 0, 'freed_memory_mb': 0}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –®–ê–†–î–ò–†–û–í–ê–ù–ù–û–ì–û —Å–µ—Ä–≤–∏—Å–∞
task_service = ShardedTaskService()
